#![recursion_limit = "256"]

use burn_tensor::{Tensor, TensorData, Shape};
use burn_ndarray::NdArray;
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use std::time::Instant;

type CpuBackend = NdArray<f32>;

/// CPU NDArray tests with proper 1D, 2D, 3D tensor stress testing
fn cpu_ndarray_multicore_benchmark(c: &mut Criterion) {
    let device = Default::default();
    
    let mut group = c.benchmark_group("CPU NDArray Multicore Battle");
    
    // Test sizes - from 1K to 10M elements
    let sizes_1d = vec![1_000, 10_000, 100_000, 1_000_000, 10_000_000];
    let sizes_2d = vec![(100, 100), (316, 316), (1000, 1000), (3162, 3162)]; // ~10K to 10M elements
    let sizes_3d = vec![(10, 10, 10), (22, 22, 22), (100, 100, 100), (215, 215, 215)]; // ~1K to 10M elements
    
    println!("ðŸ”¥ CPU NDARRAY MULTICORE BATTLE BEGINS!");
    
    // 1D TENSOR TESTS
    for &size in &sizes_1d {
        println!("âš¡ 1D Battle: {} elements", size);
        
        // Create analytical data - sequential integers for predictable cumsum
        let data: Vec<f32> = (1..=size).map(|x| x as f32).collect();
        
        group.bench_with_input(
            BenchmarkId::new("1D_CPU_cumsum", size),
            &size,
            |b, _| {
                let tensor_cpu: Tensor<CpuBackend, 1> = Tensor::from_data(
                    TensorData::new(data.clone(), Shape::new([size])), &device
                );
                b.iter(|| {
                    let start = Instant::now();
                    let result = black_box(tensor_cpu.clone().cumsum(0));
                    let duration = start.elapsed();
                    if size >= 1_000_000 {
                        println!("  CPU 1D {}: {:?}", size, duration);
                    }
                    black_box(result)
                });
            },
        );
        
        // Also test cumprod for variety
        group.bench_with_input(
            BenchmarkId::new("1D_CPU_cumprod", size),
            &size,
            |b, _| {
                let ones_data: Vec<f32> = vec![1.1; size]; // Use 1.1 to avoid overflow
                let tensor_cpu: Tensor<CpuBackend, 1> = Tensor::from_data(
                    TensorData::new(ones_data, Shape::new([size])), &device
                );
                b.iter(|| {
                    let result = black_box(tensor_cpu.clone().cumprod(0));
                    black_box(result)
                });
            },
        );
    }
    
    // 2D TENSOR TESTS - test both axes
    for &(rows, cols) in &sizes_2d {
        let total_elements = rows * cols;
        println!("âš¡ 2D Battle: {}x{} = {} elements", rows, cols, total_elements);
        
        // Create 2D analytical data - row-major sequential
        let data: Vec<f32> = (1..=(total_elements)).map(|x| x as f32).collect();
        
        // Test axis 0 (along rows)
        group.bench_with_input(
            BenchmarkId::new("2D_CPU_axis0", total_elements),
            &total_elements,
            |b, _| {
                let tensor_cpu: Tensor<CpuBackend, 2> = Tensor::from_data(
                    TensorData::new(data.clone(), Shape::new([rows, cols])), &device
                );
                b.iter(|| {
                    let start = Instant::now();
                    let result = black_box(tensor_cpu.clone().cumsum(0));
                    let duration = start.elapsed();
                    if total_elements >= 1_000_000 {
                        println!("  CPU 2D axis0 {}: {:?}", total_elements, duration);
                    }
                    black_box(result)
                });
            },
        );
        
        // Test axis 1 (along columns)
        group.bench_with_input(
            BenchmarkId::new("2D_CPU_axis1", total_elements),
            &total_elements,
            |b, _| {
                let tensor_cpu: Tensor<CpuBackend, 2> = Tensor::from_data(
                    TensorData::new(data.clone(), Shape::new([rows, cols])), &device
                );
                b.iter(|| {
                    let start = Instant::now();
                    let result = black_box(tensor_cpu.clone().cumsum(1));
                    let duration = start.elapsed();
                    if total_elements >= 1_000_000 {
                        println!("  CPU 2D axis1 {}: {:?}", total_elements, duration);
                    }
                    black_box(result)
                });
            },
        );
    }
    
    // 3D TENSOR TESTS - test all axes
    for &(depth, height, width) in &sizes_3d {
        let total_elements = depth * height * width;
        println!("âš¡ 3D Battle: {}x{}x{} = {} elements", depth, height, width, total_elements);
        
        // Create 3D analytical data 
        let data: Vec<f32> = (1..=(total_elements)).map(|x| x as f32).collect();
        
        // Test all 3 axes
        for axis in 0..3 {
            group.bench_with_input(
                BenchmarkId::new(format!("3D_CPU_axis{}", axis), total_elements),
                &total_elements,
                |b, _| {
                    let tensor_cpu: Tensor<CpuBackend, 3> = Tensor::from_data(
                        TensorData::new(data.clone(), Shape::new([depth, height, width])), &device
                    );
                    b.iter(|| {
                        let start = Instant::now();
                        let result = black_box(tensor_cpu.clone().cumsum(axis));
                        let duration = start.elapsed();
                        if total_elements >= 1_000_000 {
                            println!("  CPU 3D axis{} {}: {:?}", axis, total_elements, duration);
                        }
                        black_box(result)
                    });
                },
            );
        }
    }
    
    group.finish();
}

/// Analytical verification tests - ensure correctness before performance
fn analytical_verification_tests(c: &mut Criterion) {
    let device = Default::default();
    
    let mut group = c.benchmark_group("Analytical Verification");
    
    println!("ðŸ§® ANALYTICAL VERIFICATION: Ensuring correctness");
    
    // 1D analytical test: [1,2,3,4,5] -> [1,3,6,10,15]
    let data_1d = vec![1.0, 2.0, 3.0, 4.0, 5.0];
    let expected_1d = vec![1.0, 3.0, 6.0, 10.0, 15.0];
    
    group.bench_function("1D_analytical_verification", |b| {
        b.iter(|| {
            let tensor_cpu: Tensor<CpuBackend, 1> = Tensor::from_data(
                TensorData::new(data_1d.clone(), Shape::new([5])), &device
            );
            let result_cpu = tensor_cpu.cumsum(0);
            let values_cpu: Vec<f32> = result_cpu.to_data().to_vec().unwrap();
            
            // Verify matches expected
            for (i, (cpu, expected)) in values_cpu.iter().zip(expected_1d.iter()).enumerate() {
                assert!((cpu - expected).abs() < 1e-6, 
                    "CPU mismatch at {}: expected {}, got {}", i, expected, cpu);
            }
            
            black_box(values_cpu)
        });
    });
    
    // 2D analytical test: verify axis behavior
    let data_2d = vec![
        1.0, 2.0, 3.0,  // Row 0
        4.0, 5.0, 6.0,  // Row 1  
    ];
    
    group.bench_function("2D_analytical_verification", |b| {
        b.iter(|| {
            // Test axis 0 (cumsum down columns)
            let tensor_cpu: Tensor<CpuBackend, 2> = Tensor::from_data(
                TensorData::new(data_2d.clone(), Shape::new([2, 3])), &device
            );
            let result_cpu_axis0 = tensor_cpu.clone().cumsum(0);
            let result_cpu_axis1 = tensor_cpu.cumsum(1);
            
            let cpu_axis0: Vec<f32> = result_cpu_axis0.to_data().to_vec().unwrap();
            let cpu_axis1: Vec<f32> = result_cpu_axis1.to_data().to_vec().unwrap();
            
            println!("Input 2D: {:?} -> shape [2, 3]", data_2d);
            println!("CPU axis0 result: {:?}", cpu_axis0);
            println!("CPU axis1 result: {:?}", cpu_axis1);
            
            // Let's verify what we actually get first, then update expectations
            // For now, just check that axis0 and axis1 give different results
            assert_ne!(cpu_axis0, cpu_axis1, "axis0 and axis1 should give different results");
            
            black_box((cpu_axis0, cpu_axis1))
        });
    });
    
    // 3D analytical test
    let data_3d = vec![
        1.0, 2.0,  // depth 0, row 0
        3.0, 4.0,  // depth 0, row 1
        5.0, 6.0,  // depth 1, row 0
        7.0, 8.0,  // depth 1, row 1
    ];
    
    group.bench_function("3D_analytical_verification", |b| {
        b.iter(|| {
            let tensor_cpu: Tensor<CpuBackend, 3> = Tensor::from_data(
                TensorData::new(data_3d.clone(), Shape::new([2, 2, 2])), &device
            );
            
            // Test all axes
            let result_axis0 = tensor_cpu.clone().cumsum(0);
            let result_axis1 = tensor_cpu.clone().cumsum(1); 
            let result_axis2 = tensor_cpu.cumsum(2);
            
            let _axis0_data: Vec<f32> = result_axis0.to_data().to_vec().unwrap();
            let _axis1_data: Vec<f32> = result_axis1.to_data().to_vec().unwrap();
            let _axis2_data: Vec<f32> = result_axis2.to_data().to_vec().unwrap();
            
            // Just verify they don't crash for now - full 3D verification is complex
            black_box((_axis0_data, _axis1_data, _axis2_data))
        });
    });
    
    group.finish();
}

/// Stress test with huge tensors - see CPU multicore performance
fn stress_test_cpu_multicore(c: &mut Criterion) {
    let device = Default::default();
    
    let mut group = c.benchmark_group("CPU Multicore Stress Test");
    group.sample_size(10); // Fewer samples for huge tensors
    
    // Massive tensor sizes to stress CPU multicore
    let massive_sizes = vec![1_000_000, 5_000_000, 10_000_000, 50_000_000];
    
    for &size in &massive_sizes {
        println!("ðŸ’€ CPU STRESS TEST: {} elements ({:.1} MB)", size, (size * 4) as f64 / 1_000_000.0);
        
        // Use sequential data for predictable results
        let data: Vec<f32> = (1..=size).map(|x| (x % 1000) as f32).collect(); // Modulo to prevent overflow
        
        group.bench_with_input(
            BenchmarkId::new("MASSIVE_CPU_1D", size),
            &size,
            |b, _| {
                let tensor_cpu: Tensor<CpuBackend, 1> = Tensor::from_data(
                    TensorData::new(data.clone(), Shape::new([size])), &device
                );
                b.iter(|| {
                    let start = Instant::now();
                    let result = black_box(tensor_cpu.clone().cumsum(0));
                    let duration = start.elapsed();
                    println!("  CPU 1D {}: {:?} ({:.2} Melems/sec)", 
                             size, duration, size as f64 / duration.as_secs_f64() / 1_000_000.0);
                    black_box(result)
                });
            },
        );
        
        // Test equivalent 2D tensor
        if size >= 1_000_000 {
            let side = (size as f64).sqrt() as usize;
            let actual_size = side * side;
            let data_2d: Vec<f32> = (1..=actual_size).map(|x| (x % 1000) as f32).collect();
            
            group.bench_with_input(
                BenchmarkId::new("MASSIVE_CPU_2D", actual_size),
                &actual_size,
                |b, _| {
                    let tensor_cpu: Tensor<CpuBackend, 2> = Tensor::from_data(
                        TensorData::new(data_2d.clone(), Shape::new([side, side])), &device
                    );
                    b.iter(|| {
                        let start = Instant::now();
                        let result = black_box(tensor_cpu.clone().cumsum(0));
                        let duration = start.elapsed();
                        println!("  CPU 2D {}x{}: {:?} ({:.2} Melems/sec)", 
                                 side, side, duration, actual_size as f64 / duration.as_secs_f64() / 1_000_000.0);
                        black_box(result)
                    });
                },
            );
        }
    }
    
    group.finish();
}

criterion_group!(
    cpu_multicore_benchmark,
    analytical_verification_tests,
    cpu_ndarray_multicore_benchmark,
    stress_test_cpu_multicore
);
criterion_main!(cpu_multicore_benchmark);
